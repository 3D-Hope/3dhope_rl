defaults:
  - scene_diffuser_base

# Note: DDIM scheduler is recommended for RL training to prevent OOM problems.
ddpo:
  use_non_penetration_reward: False
  use_object_number_reward: False
  use_prompt_following_reward: False
  use_physical_feasible_objects_reward: False
  use_iou_reward: False
  use_custom_non_penetration_reward: False
  use_has_sofa_reward: False
  use_universal_reward: False  # Use composite reward with multiple physics constraints
  dynamic_constraint_rewards:
    use: False
    reward_code_dir: "/media/ajad/YourBook/AshokSaugatResearchBackup/AshokSaugatResearch/steerable-scene-generation/dynamic_constraint_rewards/dynamic_reward_functions"
    user_query: "a kids room"
    room_type: "bedroom"
    stats_path: "/media/ajad/YourBook/AshokSaugatResearchBackup/AshokSaugatResearch/steerable-scene-generation/dynamic_constraint_rewards/stats.json"
  # The RL batch size, independent of the one specified in the experiment config.
  batch_size: 32
  # Only compute policy gradients on the last n timesteps. Set to 0 to use all timesteps.
  # Note that this requires `DDPStrategy(find_unused_parameters=True)` for distributed
  # training.
  last_n_timesteps_only: 0
  # Uniformly sample this many timesteps for gradient computation. Set to 0 to use all
  # timesteps. Mutually exclusive with last_n_timesteps_only. Note that this requires
  # `DDPStrategy(find_unused_parameters=True)` for distributed training.
  n_timesteps_to_sample: 0
  advantage_max: 5.0 # Maximum advantage value
  num_reward_workers: 1
  ppo:
    num_epochs: 4 # Number of updates before sampling new data
    clip_range: 1e-4
  # The total loss is rl_loss + ddpm_reg_weight * ddpm_loss.
  # See https://arxiv.org/abs/2401.12244 for this regularization technique. Note that
  # this uses the batch size specified in the experiment config.
  ddpm_reg_weight: 500.0
  # Used when `use_physical_feasible_objects_reward` is True.
  physical_feasibility:
    non_penetration_threshold: -1.0e-3 # Allow 1mm penetration.
    use_sim: True
    sim_duration: 0.1
    sim_time_step: 1.0e-3
    sim_translation_threshold: 1.0e-3
    sim_rotation_threshold: 1.0e-2
    static_equilibrium_distance_threshold: 1.0e-3 # 1mm
  
  # Used when `use_universal_reward` is True.
  # Composite reward combines multiple physics-based rewards with importance weighting.
  universal_reward:
    # Room type for must-have furniture reward ('bedroom', 'living_room', etc.)
    room_type: 'bedroom'
    
    # Importance weights (1.0 = baseline, >1.0 = more important, <1.0 = less important)
    # These are applied AFTER normalization to [-1, 0] range.
    # All rewards are normalized first, then weighted by importance.
    importance_weights:
      must_have_furniture: 1.0  # Highest: room must be functional
      gravity: 1.0              # Critical: no floating furniture
      non_penetration: 1.0      # Critical: no overlapping objects
      object_count: 1.0         # Important: realistic clutter level

# Whether to only diffuse over the continuous/ discrete part while taking the other part
# from the dataset.
continuous_discrete_only:
  continuous_only: False
  discrete_only: False


custom:
  use: true
  num_classes: 22
  