defaults:
  - scene_diffuser_base

# Note: DDIM scheduler is recommended for RL training to prevent OOM problems.
ddpo:
  use_non_penetration_reward: False
  use_object_number_reward: False
  use_prompt_following_reward: False
  use_physical_feasible_objects_reward: False
  use_iou_reward: False
  use_custom_non_penetration_reward: False
  use_has_sofa_reward: False
  use_composite_reward: False  # Use composite reward with multiple physics constraints
  use_composite_plus_task_reward: False  # Use composite reward + task-specific reward
  # The RL batch size, independent of the one specified in the experiment config.
  batch_size: 32
  # Only compute policy gradients on the last n timesteps. Set to 0 to use all timesteps.
  # Note that this requires `DDPStrategy(find_unused_parameters=True)` for distributed
  # training.
  last_n_timesteps_only: 0
  # Uniformly sample this many timesteps for gradient computation. Set to 0 to use all
  # timesteps. Mutually exclusive with last_n_timesteps_only. Note that this requires
  # `DDPStrategy(find_unused_parameters=True)` for distributed training.
  n_timesteps_to_sample: 0
  advantage_max: 5.0 # Maximum advantage value
  num_reward_workers: 1
  ppo:
    num_epochs: 4 # Number of updates before sampling new data
    clip_range: 1e-4
  # The total loss is rl_loss + ddpm_reg_weight * ddpm_loss.
  # See https://arxiv.org/abs/2401.12244 for this regularization technique. Note that
  # this uses the batch size specified in the experiment config.
  ddpm_reg_weight: 500.0
  # Used when `use_physical_feasible_objects_reward` is True.
  physical_feasibility:
    non_penetration_threshold: -1.0e-3 # Allow 1mm penetration.
    use_sim: True
    sim_duration: 0.1
    sim_time_step: 1.0e-3
    sim_translation_threshold: 1.0e-3
    sim_rotation_threshold: 1.0e-2
    static_equilibrium_distance_threshold: 1.0e-3 # 1mm
  
  # Used when `use_composite_reward` is True.
  # Composite reward combines multiple physics-based rewards with importance weighting.
  composite_reward:
    # Room type for must-have furniture reward ('bedroom', 'living_room', etc.)
    room_type: 'bedroom'
    
    # Importance weights (1.0 = baseline, >1.0 = more important, <1.0 = less important)
    # These are applied AFTER normalization to [-1, 0] range.
    # All rewards are normalized first, then weighted by importance.
    importance_weights:
      must_have_furniture: 1.5  # Highest: room must be functional
      gravity: 1.0              # Critical: no floating furniture
      non_penetration: 1.0      # Critical: no overlapping objects
      object_count: 0.7         # Important: realistic clutter level
  
  # Used when `use_composite_plus_task_reward` is True.
  # Combines composite reward (general scene quality) with task-specific reward.
  composite_plus_task:
    # Task-specific reward type: 'has_sofa', 'has_table', 'two_beds', etc.
    task_reward_type: 'has_sofa'
    
    # Weight for task-specific reward (composite reward is weighted as sum of importance_weights)
    # Final reward = composite_reward + task_weight * task_reward
    task_weight: 1.0  # How much to emphasize task completion vs general quality
    
    # Inherit composite reward settings (room_type and importance_weights from above)
    # Or override them here:
    room_type: 'bedroom'  # Override for task-specific scenarios
    importance_weights:
      must_have_furniture: 1.5  # Lower since we have explicit task reward
      gravity: 1.0
      non_penetration: 1.0
      object_count: 0.7

# Whether to only diffuse over the continuous/ discrete part while taking the other part
# from the dataset.
continuous_discrete_only:
  continuous_only: False
  discrete_only: False


custom:
  use: true
  num_classes: 22
  