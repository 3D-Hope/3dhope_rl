import os

import torch
import torch.nn as nn

from datasets import Dataset


class MinMaxScaler(nn.Module):
    """Applies MinMax Scaling Across each input feature."""

    def __init__(
        self,
        output_min: float = 0.0,
        output_max: float = 1.0,
        clip: bool = False,
        epsilon: float = 1e-8,
    ):
        """
        Args:
            output_min (float): The minimum value of the output range.
            output_max (float): The maximum value of the output range.
            clip (bool): If True, clips the output to the output range.
            epsilon (float): Small value to add to zero ranges.
        """
        super().__init__()

        self.output_min = output_min
        self.output_max = output_max
        self.clip = clip
        self.epsilon = epsilon

        self.params = nn.ParameterDict()
        self.is_fitted = False

    def fit(self, x: torch.Tensor) -> None:
        if self.is_fitted:
            self.params.clear()

        x = x.detach().cpu()

        data_min = x.min(dim=0).values
        data_max = x.max(dim=0).values
        data_range = data_max - data_min
        data_range[data_range == 0] = self.epsilon  # Add epsilon to zero ranges.
        scale = (self.output_max - self.output_min) / data_range
        scale[scale == 0] = self.epsilon  # Add epsilon to zero scales.
        min_val = self.output_min - data_min * scale

        # Save data.
        self.params["scale"] = scale
        self.params["min"] = min_val
        self.params["input_stats"] = nn.ParameterDict(
            {
                "min": data_min,
                "max": data_max,
                "data_range": data_range,
            }
        )
        for p in self.params.parameters():
            p.requires_grad_(False)

        self.is_fitted = True

    def transform(self, x: torch.Tensor) -> torch.Tensor:
        assert self.is_fitted

        x_transformed = x * self.params["scale"].to(x.device) + self.params["min"].to(
            x.device
        )
        if self.clip:
            x_transformed = torch.clamp(x_transformed, self.output_min, self.output_max)
        return x_transformed

    def inverse_transform(self, x: torch.Tensor) -> torch.Tensor:
        assert self.is_fitted

        x_inverse = (x - self.params["min"].to(x.device)) / self.params["scale"].to(
            x.device
        )
        return x_inverse

    def get_state(self) -> dict:
        """Gets the normalizer state for torch checkpointing."""

        def detach_params(params):
            return {
                k: v.detach().cpu() if isinstance(v, nn.Parameter) else detach_params(v)
                for k, v in params.items()
            }

        return {
            "params": detach_params(self.params),
            "is_fitted": self.is_fitted,
        }

    def load_state(self, state: dict) -> None:
        """Loads the state as generated by `get_state`."""

        def create_params(params):
            return nn.ParameterDict(
                {
                    k: (
                        nn.Parameter(v)
                        if isinstance(v, torch.Tensor)
                        else create_params(v)
                    )
                    for k, v in params.items()
                }
            )

        self.params.clear()
        self.params = create_params(state["params"])
        self.is_fitted = state["is_fitted"]
        for p in self.params.parameters():
            p.requires_grad_(False)

    def get_serializable_state(self) -> dict:
        """Gets the normalizer state for json serialization."""

        def serialize_params(params):
            return {
                k: (
                    v.detach().cpu().tolist()
                    if isinstance(v, nn.Parameter)
                    else serialize_params(v)
                )
                for k, v in params.items()
            }

        return {
            "params": serialize_params(self.params),
            "is_fitted": self.is_fitted,
        }

    def load_serializable_state(self, state: dict) -> None:
        """Loads the state as generated by `get_serializable_state`."""

        def deserialize_params(params):
            return nn.ParameterDict(
                {
                    k: (
                        nn.Parameter(torch.tensor(v, dtype=torch.float32))
                        if isinstance(v, list)
                        else deserialize_params(v)
                    )
                    for k, v in params.items()
                }
            )

        self.params.clear()
        self.params = deserialize_params(state["params"])
        self.is_fitted = state["is_fitted"]
        for p in self.params.parameters():
            p.requires_grad_(False)


def fit_normalizer(scenes: torch.Tensor) -> tuple[MinMaxScaler, dict]:
    """
    Fits a normalizer to the scenes and returns its state.

    Args:
        scenes (torch.Tensor): The scenes to fit the normalizer to of shape (B, N, V).

    Returns:
        tuple: A tuple containing the fitted MinMaxScaler and its serializable state.
    """
    normalizer = MinMaxScaler(output_min=-1.0, output_max=1.0, clip=True)

    # Flatten scenes into object vectors and normalize separately across each object
    # vector feature.
    normalizer.fit(scenes.reshape(-1, scenes.shape[-1]))  # Shape (B*N, V)

    state = normalizer.get_serializable_state()
    return normalizer, state


def fit_normalizer_hf(
    hf_dataset: Dataset,
    batch_size: int = 1000,
    num_proc: int = os.cpu_count(),
    scene_key: str = "scenes",
) -> tuple[MinMaxScaler, dict]:
    """
    Fits a normalizer to scenes in a HuggingFace dataset without loading the entire
    dataset into memory.

    Args:
        hf_dataset: The HuggingFace dataset containing scenes.
        batch_size (int): Number of scenes to process in each batch.
        num_proc (int, optional): Number of processes to use for parallel processing.
        scene_key (str): The key in the dataset that contains the scene tensors.

    Returns:
        tuple: A tuple containing the fitted MinMaxScaler and its serializable state.
    """
    normalizer = MinMaxScaler(output_min=-1.0, output_max=1.0, clip=True)

    # Ensure dataset is in torch format.
    hf_dataset.set_format("torch")

    # Get the shape of a single scene to determine feature dimension.
    sample_scene = hf_dataset[0][scene_key]  # Shape (N, V)
    feature_dim = sample_scene.shape[-1]

    # Function to compute min and max for a batch.
    def compute_batch_stats(
        examples: dict[str, torch.Tensor],
    ) -> dict[str, torch.Tensor]:
        scenes = examples[scene_key]  # Shape (B, N, V)
        flat_scenes = scenes.reshape(-1, feature_dim)  # Shape (B*N, V)

        # Unsqueeze for correct batched dataset format.
        batch_min = flat_scenes.min(dim=0).values.unsqueeze(0)  # Shape (1,V)
        batch_max = flat_scenes.max(dim=0).values.unsqueeze(0)  # Shape (1,V)

        return {"batch_min": batch_min, "batch_max": batch_max}

    # Process dataset in parallel to find min/max for each batch.
    stats_dataset = hf_dataset.map(
        compute_batch_stats,
        batched=True,
        batch_size=batch_size,
        num_proc=num_proc,
        remove_columns=hf_dataset.column_names,
        desc="Computing min/max for each batch",
    )

    # Aggregate results from all batches.
    data_min = torch.full((feature_dim,), float("inf"))  # Shape (V,)
    data_max = torch.full((feature_dim,), float("-inf"))  # Shape (V,)
    for batch_stats in stats_dataset:
        data_min = torch.minimum(data_min, batch_stats["batch_min"])  # Shape (V,)
        data_max = torch.maximum(data_max, batch_stats["batch_max"])  # Shape (V,)

    # Calculate range.
    data_range = data_max - data_min
    data_range[data_range == 0] = normalizer.epsilon  # Add epsilon to zero ranges

    # Calculate scale and min values.
    scale = (normalizer.output_max - normalizer.output_min) / data_range
    scale[scale == 0] = normalizer.epsilon  # Add epsilon to zero scales
    min_val = normalizer.output_min - data_min * scale

    # Save parameters to normalizer.
    normalizer.params["scale"] = nn.Parameter(scale, requires_grad=False)
    normalizer.params["min"] = nn.Parameter(min_val, requires_grad=False)
    normalizer.params["input_stats"] = nn.ParameterDict(
        {
            "min": nn.Parameter(data_min, requires_grad=False),
            "max": nn.Parameter(data_max, requires_grad=False),
            "data_range": nn.Parameter(data_range, requires_grad=False),
        }
    )

    normalizer.is_fitted = True

    state = normalizer.get_serializable_state()
    return normalizer, state
